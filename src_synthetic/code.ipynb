{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perceptron():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(1, input_shape=(2,))\n",
    "    ])\n",
    "\n",
    "\n",
    "def tff_perceptron_model_fn():\n",
    "    # We _must_ create a new model here, and _not_ capture it from an external\n",
    "    # scope. TFF will call this within different graph contexts.\n",
    "    keras_model = get_perceptron()\n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model,\n",
    "        input_spec=(tf.TensorSpec(shape=[None, 2], dtype=tf.float64),\n",
    "            tf.TensorSpec(shape=[None,], dtype=tf.float64)),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.0)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = tff_perceptron_model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SERVER_STATE = {model weights}\n",
    "@tff.tf_computation\n",
    "def server_init():\n",
    "    model = model_fn()\n",
    "    return model.trainable_variables\n",
    "\n",
    "# {model weights}@SERVER\n",
    "@tff.federated_computation\n",
    "def initialize_fn():\n",
    "    return tff.federated_value(server_init(), tff.SERVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining type signatures - 1\n",
    "model_weights_type = server_init.type_signature.result\n",
    "dummy_model = model_fn()\n",
    "tf_dataset_type = tff.SequenceType(dummy_model.input_spec)\n",
    "federated_server_state_type = tff.FederatedType(model_weights_type, tff.SERVER)\n",
    "federated_dataset_type = tff.FederatedType(tf_dataset_type, tff.CLIENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def client_update(model, dataset, server_weights, lr, accumulator):\n",
    "    \n",
    "    \"\"\"Performs training (using the server model weights) on the client's dataset.\"\"\"\n",
    "    # Initialize the client model with the current server weights.\n",
    "    client_weights = model.trainable_variables\n",
    "    # Assign the server weights to the client model.\n",
    "    tf.nest.map_structure(lambda x, y: x.assign(y),\n",
    "                        client_weights, server_weights)\n",
    "\n",
    "    # Update the local model using SGD.\n",
    "    for batch in dataset:\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Compute a forward pass on the batch of data\n",
    "            outputs = model.forward_pass(batch)\n",
    "\n",
    "        # Compute the corresponding gradient\n",
    "        grads = tape.gradient(outputs.loss, client_weights)\n",
    "\n",
    "        # Apply the gradient using a client optimizer.\n",
    "        updated_accumulator = tf.nest.map_structure(lambda a, g: -lr*g, accumulator, grads)\n",
    "        updated_weights = tf.nest.map_structure(lambda w, a: w+a, client_weights, updated_accumulator)\n",
    "        \n",
    "        tf.nest.map_structure(lambda x, y: x.assign(y), client_weights, updated_weights)\n",
    "        tf.nest.map_structure(lambda x, y: x.assign(y), accumulator, updated_accumulator)\n",
    "    \n",
    "    # dictionary containing  {metric: [sum, count], ..}\n",
    "    out_data = model.report_local_outputs()\n",
    "    \n",
    "    return client_weights, out_data, out_data['loss'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tff.tf_computation(tf_dataset_type, model_weights_type, tf.float32)\n",
    "def client_update_fn(tf_dataset, server_weights_at_client, learning_rate):\n",
    "    model = model_fn()\n",
    "    \n",
    "    # To be used when optimizer is SGD with momentum\n",
    "    accumulator = tf.nest.map_structure(lambda l: tf.Variable(tf.zeros(l.shape, l.dtype)), server_weights_at_client)\n",
    "\n",
    "    client_weights, out_data, n = client_update(model, tf_dataset, \n",
    "        server_weights_at_client, learning_rate, accumulator)\n",
    "\n",
    "    return client_weights, out_data, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def server_update(model, mean_client_weights):\n",
    "    \"\"\"Updates the server model weights as the average of the client model weights.\"\"\"\n",
    "    model_weights = model.trainable_variables\n",
    "    # Assign the mean client weights to the server model.\n",
    "    tf.nest.map_structure(lambda x, y: x.assign(y),\n",
    "                        model_weights, mean_client_weights)\n",
    "    return model_weights\n",
    "\n",
    "@tff.tf_computation(model_weights_type)\n",
    "def server_update_fn(mean_client_weights):\n",
    "    model = model_fn()\n",
    "    return server_update(model, mean_client_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining type signatures - 2\n",
    "client_learning_rates_type = tff.FederatedType(tf.float32, tff.CLIENTS, all_equal=False)\n",
    "client_agg_weights_type = tff.FederatedType(tf.float32, tff.CLIENTS, all_equal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for data extraction and processing\n",
    "@tff.tf_computation(client_update_fn.type_signature.result)\n",
    "def extract_weights(tp_wts_mts):\n",
    "    return tp_wts_mts[0], tp_wts_mts[2] \n",
    "\n",
    "@tff.tf_computation(client_update_fn.type_signature.result)\n",
    "def extract_only_weights(tp_wts_mts):\n",
    "  return tp_wts_mts[0]\n",
    "\n",
    "@tff.tf_computation(client_update_fn.type_signature.result)\n",
    "def extract_training_metrics(tp_wts_mts):\n",
    "    return tp_wts_mts[1]\n",
    "\n",
    "# Receives a dictionary {metric: [sum_all_samples, total_samples]}\n",
    "# Return dictionary of means for every metric\n",
    "@tff.tf_computation(client_update_fn.type_signature.result[1])\n",
    "def get_mean(metric_dict):\n",
    "    d = {}\n",
    "    for k,v in metric_dict.items():\n",
    "        d[k] = v[0]/v[1] \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tff.federated_computation(\n",
    "    federated_server_state_type, \n",
    "    federated_dataset_type, \n",
    "    client_learning_rates_type, \n",
    "    client_agg_weights_type) # List of p_i = n_i/n\n",
    "def next_fn(\n",
    "    server_state, \n",
    "    federated_dataset, \n",
    "    client_learning_rates,\n",
    "    client_agg_weights):\n",
    "    \n",
    "    # Broadcast the server weights to the clients.\n",
    "    server_weights_at_clients = tff.federated_broadcast(server_state)\n",
    "\n",
    "    # Each client computes their updated weights.\n",
    "    # Epochs and lr supplied by orchestrator (i.e us) \n",
    "    # instead of server for the purposes of simulation\n",
    "    client_weights_and_metrics = tff.federated_map(\n",
    "        client_update_fn, (federated_dataset, server_weights_at_clients, client_learning_rates))\n",
    "    \n",
    "    client_weights = tff.federated_map(extract_only_weights, client_weights_and_metrics)\n",
    "    client_metrics = tff.federated_map(extract_training_metrics, client_weights_and_metrics)\n",
    "    \n",
    "    # Weighted averaging of client models - sum p_i x w_i\n",
    "    mean_client_weights = tff.federated_mean(client_weights, client_agg_weights)\n",
    "    \n",
    "    # compute mean of training metrics\n",
    "    client_metrics_summed = tff.federated_sum(client_metrics)\n",
    "    mean_metrics = tff.federated_map(get_mean, client_metrics_summed)\n",
    "\n",
    "    # The server updates its model.\n",
    "    server_state = tff.federated_map(server_update_fn, mean_client_weights)\n",
    "\n",
    "    return server_state, mean_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from math import floor, ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SyntheticData:\n",
    "    def __init__(self, train_dir, test_dir):\n",
    "        with open(os.path.join(train_dir, 'train.json')) as f:\n",
    "            train_d = json.load(f)\n",
    "        \n",
    "        self.client_ids, self.num_samples, self.train_data = train_d['users'], train_d['num_samples'], train_d['user_data']\n",
    "        \n",
    "        with open(os.path.join(test_dir, 'test.json')) as f:\n",
    "            test_d = json.load(f)\n",
    "        \n",
    "        self.test_data = test_d['user_data']\n",
    "\n",
    "    def get_client_ids(self):\n",
    "        return self.client_ids\n",
    "\n",
    "    def create_dataset_for_client(self, client_id):\n",
    "        client_data = self.train_data[client_id]\n",
    "        return tf.data.Dataset.from_tensor_slices((client_data['x'], client_data['y'])).map(lambda a,b: (tf.cast(a, tf.float64), tf.cast(b, tf.float64)))\n",
    "\n",
    "    def create_train_dataset_for_all_clients(self):\n",
    "        xs = list()\n",
    "        ys = list()\n",
    "        for data in self.train_data.values():\n",
    "            for x in data['x']:\n",
    "                xs.append(x)\n",
    "            for y in data['y']:\n",
    "                ys.append(y)\n",
    "        xs = np.array(xs)\n",
    "        ys = np.array(ys)\n",
    "        return tf.data.Dataset.from_tensor_slices((xs, ys))\n",
    "\n",
    "    def create_test_dataset_for_all_clients(self):\n",
    "        xs = list()\n",
    "        ys = list()\n",
    "        for data in self.test_data.values():\n",
    "            for x in data['x']:\n",
    "                xs.append(x)\n",
    "            for y in data['y']:\n",
    "                ys.append(y)\n",
    "        xs = np.array(xs)\n",
    "        ys = np.array(ys)\n",
    "        return tf.data.Dataset.from_tensor_slices((xs, ys)).map(lambda a,b: (tf.cast(a, tf.float64), tf.cast(b, tf.float64)))\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "def make_federated_data(dataset, preprocess_fn, client_ids, client_num_samples, client_capacities, batch_size, round_num):\n",
    "    return [\n",
    "      preprocess_fn(dataset.create_dataset_for_client(client_ids[i]), batch_size, client_capacities[i], client_num_samples[i], round_num)\n",
    "      for i in range(len((client_ids)))\n",
    "    ]\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "def preprocess(dataset, b, u, n, r):\n",
    "    u_p = floor(n/b)\n",
    "    if(u <= u_p):\n",
    "        return dataset.shuffle(n, seed=r).batch(b).take(u)\n",
    "    else:\n",
    "        x = ceil((b*u)/n)\n",
    "        return dataset.repeat(x).shuffle(n, seed=r).batch(b).take(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_agg_weights(budgets, client_num_samples, batch_size):\n",
    "    seen_training_samples = []\n",
    "    tp_i = []\n",
    "    for i in range(len(budgets)):\n",
    "        tau_i = budgets[i]\n",
    "        n_i = client_num_samples[i]\n",
    "        tau_possible_i = floor(n_i / batch_size)\n",
    "        tp_i.append(tau_possible_i)\n",
    "        if(tau_i <= tau_possible_i):\n",
    "            n_final_i = tau_i * batch_size\n",
    "        else:\n",
    "            n_final_i = n_i\n",
    "        seen_training_samples.append(n_final_i)\n",
    "\n",
    "    total_seen_training_samples = sum(seen_training_samples)\n",
    "    agg_weights = [\n",
    "        x/total_seen_training_samples for x in seen_training_samples]\n",
    "\n",
    "    return agg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_process = tff.templates.IterativeProcess(\n",
    "    initialize_fn=initialize_fn,\n",
    "    next_fn=next_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/mnt/nfs/dhasade/optml/data/synthetic_data/train'\n",
    "test_dir = '/mnt/nfs/dhasade/optml/data/synthetic_data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SyntheticData(train_dir, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_clients = len(dataset.num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = 2; upper_bound = 2; num_clients = 5; lr_schedule=lambda round_num: 0.5/(1 + round_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = iterative_process.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train accuracy 0.72 ]\n",
      "[Train loss 0.48016942 ]\n",
      "[Train accuracy 0.72 ]\n",
      "[Train loss 0.4644083 ]\n",
      "[Train accuracy 0.88 ]\n",
      "[Train loss 0.32609066 ]\n",
      "[Train accuracy 0.82 ]\n",
      "[Train loss 0.33324128 ]\n",
      "[Train accuracy 0.86 ]\n",
      "[Train loss 0.38307858 ]\n",
      "[Train accuracy 0.78000003 ]\n",
      "[Train loss 0.41607076 ]\n",
      "[Train accuracy 0.78000003 ]\n",
      "[Train loss 0.45950142 ]\n",
      "[Train accuracy 0.78000003 ]\n",
      "[Train loss 0.42677802 ]\n",
      "[Train accuracy 0.82 ]\n",
      "[Train loss 0.3205042 ]\n",
      "[Train accuracy 0.9 ]\n",
      "[Train loss 0.38034537 ]\n",
      "[Train accuracy 0.74000007 ]\n",
      "[Train loss 0.45960006 ]\n",
      "[Train accuracy 0.82 ]\n",
      "[Train loss 0.46016908 ]\n",
      "[Train accuracy 0.94000006 ]\n",
      "[Train loss 0.31331843 ]\n",
      "[Train accuracy 0.73999995 ]\n",
      "[Train loss 0.5235738 ]\n",
      "[Train accuracy 0.85999995 ]\n",
      "[Train loss 0.4059956 ]\n",
      "[Train accuracy 0.84 ]\n",
      "[Train loss 0.38186267 ]\n",
      "[Train accuracy 0.82000005 ]\n",
      "[Train loss 0.4100829 ]\n",
      "[Train accuracy 0.91999996 ]\n",
      "[Train loss 0.358624 ]\n",
      "[Train accuracy 0.8 ]\n",
      "[Train loss 0.45920867 ]\n",
      "[Train accuracy 0.93999994 ]\n",
      "[Train loss 0.3626774 ]\n",
      "[Train accuracy 0.98 ]\n",
      "[Train loss 0.2568876 ]\n",
      "[Train accuracy 0.78000003 ]\n",
      "[Train loss 0.41913933 ]\n",
      "[Train accuracy 0.86 ]\n",
      "[Train loss 0.40199825 ]\n",
      "[Train accuracy 0.88 ]\n",
      "[Train loss 0.33998153 ]\n",
      "[Train accuracy 0.82 ]\n",
      "[Train loss 0.36254448 ]\n",
      "[Train accuracy 0.82 ]\n",
      "[Train loss 0.44173896 ]\n",
      "[Train accuracy 0.88 ]\n",
      "[Train loss 0.37890017 ]\n",
      "[Train accuracy 0.85999995 ]\n",
      "[Train loss 0.36241683 ]\n",
      "[Train accuracy 0.74 ]\n",
      "[Train loss 0.48450568 ]\n",
      "[Train accuracy 0.64000005 ]\n",
      "[Train loss 0.5836624 ]\n",
      "[Train accuracy 0.84 ]\n",
      "[Train loss 0.4179493 ]\n",
      "[Train accuracy 0.9 ]\n",
      "[Train loss 0.28840217 ]\n",
      "[Train accuracy 0.76 ]\n",
      "[Train loss 0.4837786 ]\n",
      "[Train accuracy 0.88 ]\n",
      "[Train loss 0.345271 ]\n",
      "[Train accuracy 0.70000005 ]\n",
      "[Train loss 0.41322425 ]\n",
      "[Train accuracy 0.96000004 ]\n",
      "[Train loss 0.37659687 ]\n",
      "[Train accuracy 0.76000005 ]\n",
      "[Train loss 0.39728242 ]\n",
      "[Train accuracy 0.74 ]\n",
      "[Train loss 0.50973475 ]\n",
      "[Train accuracy 0.93999994 ]\n",
      "[Train loss 0.26137477 ]\n",
      "[Train accuracy 0.82 ]\n",
      "[Train loss 0.37705705 ]\n",
      "[Train accuracy 0.74 ]\n",
      "[Train loss 0.42286855 ]\n",
      "[Train accuracy 0.73999995 ]\n",
      "[Train loss 0.42025402 ]\n",
      "[Train accuracy 0.8 ]\n",
      "[Train loss 0.46180004 ]\n",
      "[Train accuracy 0.78000003 ]\n",
      "[Train loss 0.4372356 ]\n",
      "[Train accuracy 0.76000005 ]\n",
      "[Train loss 0.48777747 ]\n",
      "[Train accuracy 0.75999993 ]\n",
      "[Train loss 0.50436777 ]\n",
      "[Train accuracy 0.9 ]\n",
      "[Train loss 0.3437753 ]\n",
      "[Train accuracy 0.84000003 ]\n",
      "[Train loss 0.4163051 ]\n",
      "[Train accuracy 0.84 ]\n",
      "[Train loss 0.41500542 ]\n",
      "[Train accuracy 0.84 ]\n",
      "[Train loss 0.4161148 ]\n",
      "[Train accuracy 0.82000005 ]\n",
      "[Train loss 0.3932295 ]\n",
      "[Train accuracy 0.74000007 ]\n",
      "[Train loss 0.4364695 ]\n",
      "[Train accuracy 0.9 ]\n",
      "[Train loss 0.3125697 ]\n",
      "[Train accuracy 0.96000004 ]\n",
      "[Train loss 0.35122544 ]\n",
      "[Train accuracy 0.84000003 ]\n",
      "[Train loss 0.40067083 ]\n",
      "[Train accuracy 0.75999993 ]\n",
      "[Train loss 0.5508501 ]\n",
      "[Train accuracy 0.72 ]\n",
      "[Train loss 0.41137004 ]\n",
      "[Train accuracy 0.82 ]\n",
      "[Train loss 0.4687902 ]\n",
      "[Train accuracy 0.86 ]\n",
      "[Train loss 0.36567202 ]\n",
      "[Train accuracy 0.68 ]\n",
      "[Train loss 0.5104288 ]\n",
      "[Train accuracy 0.94000006 ]\n",
      "[Train loss 0.31295282 ]\n",
      "[Train accuracy 0.82000005 ]\n",
      "[Train loss 0.37450776 ]\n",
      "[Train accuracy 0.82 ]\n",
      "[Train loss 0.43680763 ]\n",
      "[Train accuracy 0.86 ]\n",
      "[Train loss 0.42737797 ]\n",
      "[Train accuracy 0.8 ]\n",
      "[Train loss 0.43492088 ]\n",
      "[Train accuracy 0.66 ]\n",
      "[Train loss 0.52281594 ]\n",
      "[Train accuracy 0.76 ]\n",
      "[Train loss 0.474428 ]\n",
      "[Train accuracy 0.93999994 ]\n",
      "[Train loss 0.27612767 ]\n",
      "[Train accuracy 0.86 ]\n",
      "[Train loss 0.3524848 ]\n",
      "[Train accuracy 0.88 ]\n",
      "[Train loss 0.3705579 ]\n",
      "[Train accuracy 0.9 ]\n",
      "[Train loss 0.31965694 ]\n",
      "[Train accuracy 0.88 ]\n",
      "[Train loss 0.3404684 ]\n",
      "[Train accuracy 0.8 ]\n",
      "[Train loss 0.3770251 ]\n",
      "[Train accuracy 0.82000005 ]\n",
      "[Train loss 0.45278907 ]\n",
      "[Train accuracy 0.93999994 ]\n",
      "[Train loss 0.34357136 ]\n",
      "[Train accuracy 0.96000004 ]\n",
      "[Train loss 0.254461 ]\n",
      "[Train accuracy 0.82000005 ]\n",
      "[Train loss 0.46084756 ]\n",
      "[Train accuracy 0.76 ]\n",
      "[Train loss 0.45735177 ]\n",
      "[Train accuracy 0.82000005 ]\n",
      "[Train loss 0.39989975 ]\n",
      "[Train accuracy 0.72 ]\n",
      "[Train loss 0.51433855 ]\n",
      "[Train accuracy 0.82 ]\n",
      "[Train loss 0.5437197 ]\n",
      "[Train accuracy 0.86 ]\n",
      "[Train loss 0.35812312 ]\n",
      "[Train accuracy 0.93999994 ]\n",
      "[Train loss 0.31770247 ]\n",
      "[Train accuracy 0.88 ]\n",
      "[Train loss 0.34663504 ]\n",
      "[Train accuracy 0.85999995 ]\n",
      "[Train loss 0.40893036 ]\n",
      "[Train accuracy 0.71999997 ]\n",
      "[Train loss 0.4805767 ]\n",
      "[Train accuracy 0.86 ]\n",
      "[Train loss 0.36421597 ]\n",
      "[Train accuracy 0.74 ]\n",
      "[Train loss 0.44180083 ]\n",
      "[Train accuracy 0.84 ]\n",
      "[Train loss 0.52377164 ]\n",
      "[Train accuracy 0.87999994 ]\n",
      "[Train loss 0.27664712 ]\n",
      "[Train accuracy 0.82 ]\n",
      "[Train loss 0.4370642 ]\n",
      "[Train accuracy 0.7800001 ]\n",
      "[Train loss 0.512461 ]\n",
      "[Train accuracy 0.74000007 ]\n",
      "[Train loss 0.44484946 ]\n",
      "[Train accuracy 0.88 ]\n",
      "[Train loss 0.35052174 ]\n",
      "[Train accuracy 0.93999994 ]\n",
      "[Train loss 0.2911827 ]\n",
      "[Train accuracy 0.91999996 ]\n",
      "[Train loss 0.310674 ]\n",
      "[Train accuracy 0.86 ]\n",
      "[Train loss 0.47137505 ]\n",
      "[Train accuracy 0.86 ]\n",
      "[Train loss 0.39080724 ]\n",
      "[Train accuracy 0.96000004 ]\n",
      "[Train loss 0.33291978 ]\n",
      "[Train accuracy 0.73999995 ]\n",
      "[Train loss 0.4936489 ]\n"
     ]
    }
   ],
   "source": [
    "for round_num in range(100):\n",
    "    client_indexes = rng.integers(\n",
    "                    low=0, high=total_clients, size=num_clients)\n",
    "\n",
    "    client_ids = []\n",
    "    client_num_samples = []\n",
    "    for client_index in client_indexes:\n",
    "        client_ids.append(dataset.client_ids[client_index])\n",
    "        client_num_samples.append(dataset.num_samples[client_index])\n",
    "\n",
    "    budgets = rng.integers(\n",
    "        low=lower_bound, high=upper_bound+1, size=num_clients)\n",
    "\n",
    "    lr_to_clients = [lr_schedule(round_num)]*num_clients\n",
    "    client_agg_weights = get_client_agg_weights(\n",
    "        budgets, client_num_samples, 5)\n",
    "\n",
    "    federated_train_data = make_federated_data(\n",
    "        dataset, preprocess, client_ids, client_num_samples, budgets, 5, round_num)\n",
    "    \n",
    "    state, metrics = iterative_process.next(\n",
    "        state,\n",
    "        federated_train_data,\n",
    "        lr_to_clients,\n",
    "        client_agg_weights\n",
    "        )\n",
    "    \n",
    "    for name, value in metrics.items():\n",
    "        tf.summary.scalar('train_' + name, value, step=round_num)\n",
    "        if('loss' in name):\n",
    "            print('[Train loss', value, ']')\n",
    "        else:\n",
    "            print('[Train accuracy', value, ']')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_synthetic(server_model_weights, central_test_dataset):\n",
    "  keras_model = get_perceptron()\n",
    "  keras_model.compile(\n",
    "      loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "      metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.0)]  \n",
    "  )\n",
    "  keras_model.set_weights(server_model_weights)\n",
    "  return keras_model.evaluate(central_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "central_test_dataset = dataset.create_test_dataset_for_all_clients().batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3944 - binary_accuracy: 0.8475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3944089114665985, 0.8475000262260437]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_synthetic(state, central_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aca0000d8b95a7db30aecac74c0d64ba50d0ed041baf1a605b9aa3fc8c339d6a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
