\subsection{Problem definition}

We consider a federated optimization problem of the form:

\begin{align*}
    \underset{x}{\min}~ F(x),
\end{align*}
where
\begin{align*}
    F(x) =  \frac{1}{M}~ \sum_{i=1}^M F_i(x).
\end{align*}

The federated setting of the problem imposes that a given client $i$ can only query the function $F_i$ and that communications are allowed between the server and it's clients.


\subsection{The Federated Averaging Algorithm}

We consider the Federate Algorithm (\textit{FedAVG}) which we formally describe below.




\begin{algorithm}
    \caption{Federated Averaging}
    \label{FedAVG}
    \textbf{Input : } initial model $\bm{x}^(0)$, learning rate $\eta$ \\
    \For(){$t \in \{0,1,...,T-1\} $}{
        \For(){$i \in \{1,...,M\} $ clients in parallel}{
            Initialize local model $\bm{x}^{(t,0)}_i \leftarrow \bm{x}^{(t)}$ \\
            \For(){$k \in \{0,...,\tau\} $}{
                Compute local stochastic gradient ${g_i}(\bm{x}^{(t,k)})$ \\
                Compute local step $\bm{x}_i^{(t,k+1)} \leftarrow \bm{x}_i^{(t,k)} - \eta {g_i}(\bm{x}^{(t,k)})$
            }
            Compute local change over round $\Delta_i^{(t)} \leftarrow \bm{x}_i^{(t,\tau)} - \bm{x}_i ^{(t,0)}$
        }
        Average local updates $\Delta^{(t)} = \frac{1}{M} \cdot \sum_{i=1}^M \Delta_i^{(t)}$Â \\
        Update global model $\bm{x}^{(t+1)} \leftarrow \bm{x}^{(t)} + \Delta^{(t)}$ 
    }
    \textbf{Return :} $\bm{x}^{(t)}$
\end{algorithm}

